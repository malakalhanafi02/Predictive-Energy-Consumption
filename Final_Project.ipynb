{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "APcTCB-Y7Xfi",
        "outputId": "86b45ba0-86cb-4ba0-e379-fddd94b5827f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.10/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.10/dist-packages (from ucimlrepo) (2024.8.30)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.0->ucimlrepo) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "soj3B63dYSPv"
      },
      "outputs": [],
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# fetch dataset\n",
        "dataset = fetch_ucirepo(id=235)\n",
        "\n",
        "# data (as pandas dataframes)\n",
        "X_data = dataset.data.features\n",
        "y_data = dataset.data.targets\n",
        "\n",
        "# metadata\n",
        "print(dataset.metadata)\n",
        "\n",
        "# variable information\n",
        "print(dataset.variables)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NYfbXfegYSPy"
      },
      "outputs": [],
      "source": [
        "# Create a deep copy of the X_data from the dataset, to be preprocessed\n",
        "X = X_data.copy(deep=True)\n",
        "categories = ['Global_active_power', 'Global_reactive_power', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
        "X[categories] = X[categories].apply(pd.to_numeric, errors='coerce')\n",
        "for cat in categories:\n",
        "    X[cat] = X[cat].interpolate()\n",
        "\n",
        "np.sum(np.isnan(X['Global_active_power']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l8Kkrq27Nd1"
      },
      "outputs": [],
      "source": [
        "# Identify where the values are NaN\n",
        "is_nan = np.isnan(X['Global_active_power'])\n",
        "\n",
        "# Find where the NaN sequences start and end\n",
        "nan_runs = np.diff(np.concatenate(([0], is_nan.astype(int), [0])))\n",
        "start_indices = np.where(nan_runs == 1)[0]\n",
        "end_indices = np.where(nan_runs == -1)[0]\n",
        "\n",
        "# Calculate the lengths of each run\n",
        "nan_lengths = end_indices - start_indices\n",
        "\n",
        "# Filter runs with more than 5 NaNs\n",
        "long_nan_runs = [(start, length) for start, length in zip(start_indices, nan_lengths) if length > 5]\n",
        "\n",
        "print(long_nan_runs)\n",
        "\n",
        "X['Date'][190497+3723], X['Date'][1309386]\n",
        "X['Date'][0], X['Date'][len(X)-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD1NR5wvYSPz"
      },
      "outputs": [],
      "source": [
        "# Calculate Power Factor and put in in the Dataframe\n",
        "PF = np.cos(np.arctan(X['Global_reactive_power'] / X['Global_active_power']))\n",
        "X.insert(4, 'Power_factor', PF, True)\n",
        "X.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2jC39doYSPz"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "from matplotlib.ticker import MaxNLocator\n",
        "\n",
        "# Adjust Date_Time column for sensible plots\n",
        "DateTime = X['Date'].str.cat(X['Time'].values.astype(str), sep=' ')\n",
        "X.insert(0, 'Date_Time', DateTime, True) #includes Date_time variable\n",
        "X = X.drop('Date', axis=1) #removes date column\n",
        "X = X.drop('Time', axis=1) #removes time column\n",
        "X.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r4OCOr4V7Nd3"
      },
      "outputs": [],
      "source": [
        "# Calculate Global Active Energy\n",
        "GAE = X['Global_active_power']*(1000/60) - X['Sub_metering_1'] - X['Sub_metering_2'] - X['Sub_metering_3']\n",
        "X.insert(1, 'GAE', GAE, True)\n",
        "X.describe(include='all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Qk22wMmzC6k"
      },
      "outputs": [],
      "source": [
        "# Define categories for each figure\n",
        "fig1_categories = ['Global_active_power', 'Global_reactive_power', 'Power_factor']\n",
        "fig2_categories = ['GAE']\n",
        "fig3_categories = ['Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
        "fig4_categories = ['Voltage', 'Global_intensity']\n",
        "\n",
        "# Helper function to create and format each figure\n",
        "def create_figure(categories, fig_title):\n",
        "    fig, axes = plt.subplots(len(categories), figsize=(15, 6 * len(categories)))\n",
        "    fig.suptitle(fig_title, fontsize=16)\n",
        "\n",
        "    for i, category in enumerate(categories):\n",
        "        ax = axes[i] if len(categories) > 1 else axes\n",
        "        ax.plot(X['Date_Time'], X[category], label=category)\n",
        "        ax.set_title(category)\n",
        "        ax.set_xlabel('Time')\n",
        "        ax.set_ylabel(category)\n",
        "        ax.legend()\n",
        "\n",
        "        # Set the number of x-axis ticks to 12 and rotate labels\n",
        "        ax.xaxis.set_major_locator(MaxNLocator(13))\n",
        "        ax.tick_params(axis='x', rotation=45)\n",
        "\n",
        "        # Set the number of y-axis ticks to 5\n",
        "        ax.yaxis.set_major_locator(MaxNLocator(5))\n",
        "\n",
        "    # Adjust layout to prevent overlap\n",
        "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "# Create each figure based on the planned categories\n",
        "create_figure(fig1_categories, 'Figure 1: Global Active Power, Global Reactive Power, Power Factor')\n",
        "create_figure(fig2_categories, 'Figure 2: Global Active Energy (GAE)')\n",
        "create_figure(fig3_categories, 'Figure 3: Sub Meterings 1-3')\n",
        "create_figure(fig4_categories, 'Figure 4: Voltage and Current (Global Intensity)')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pho780CrEm5"
      },
      "outputs": [],
      "source": [
        "# Create plots for each category\n",
        "fig, axes = plt.subplots(8, figsize=(15, 20))\n",
        "fig.suptitle('First 3 Days of Each Category vs Time [Reduced Dataset]', fontsize=16)\n",
        "\n",
        "category = ['GAE', 'Global_active_power', 'Global_reactive_power','Power_factor', 'Voltage', 'Global_intensity', 'Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3']\n",
        "\n",
        "\n",
        "# Plot each category\n",
        "for i, category in enumerate(category):\n",
        "    ax = axes[i]\n",
        "    ax.plot(X['Date_Time'], X[category], label=category)\n",
        "    ax.set_title(category)\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel(category)\n",
        "    ax.legend()\n",
        "\n",
        "    # Limit the number of x-axis ticks to avoid clutter\n",
        "    ax.xaxis.set_major_locator(plt.MaxNLocator(5))  # Adjust the number as needed\n",
        "    ax.yaxis.set_major_locator(plt.MaxNLocator(5))\n",
        "\n",
        "# Adjust layout to prevent overlap\n",
        "plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuUA1OyD7Nd5"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, LSTM, GRU\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.metrics import MeanAbsoluteError\n",
        "\n",
        "from sklearn.metrics import r2_score as sklearn_r2_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def r2_score(y_true, y_pred):\n",
        "    # Cast y_true to float32 to ensure type consistency\n",
        "    y_true = K.cast(y_true, dtype='float32')\n",
        "    ss_res = K.sum(K.square(y_true - y_pred))\n",
        "    ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
        "    return 1 - ss_res / (ss_tot + K.epsilon())\n",
        "\n",
        "def custom_mse(y_true, y_pred):\n",
        "    # Calculate MSE for each output separately and average them\n",
        "    return K.mean(K.square(y_true - y_pred), axis=-1)\n",
        "\n",
        "def custom_mae(y_true, y_pred):\n",
        "    # Calculate MAE for each output separately and average them\n",
        "    return K.mean(K.abs(y_true - y_pred), axis=-1)\n",
        "\n",
        "# Preprocessing function for Date_Time\n",
        "def preprocess_datetime(data):\n",
        "    \"\"\"\n",
        "    Converts 'Date_Time' strings to Unix timestamps.\n",
        "    Adjust the format string to match your datetime format.\n",
        "    \"\"\"\n",
        "    return np.array([\n",
        "        datetime.strptime(dt, '%d/%m/%Y %H:%M:%S').timestamp() for dt in data\n",
        "    ])\n",
        "\n",
        "# Split the data\n",
        "def train_test_split(data, categories: list, predictors:str, split=0.8, fraction=1.0):\n",
        "    \"\"\"\n",
        "    Splits the given data into train and test sets.\n",
        "    If fraction is a value less than 1, than only that proportion of the data is used.\n",
        "    \"\"\"\n",
        "    assert 0 < fraction <= 1, \"Fraction must be between 0 and 1\"\n",
        "    fraction_int = int(len(data) * fraction)\n",
        "    X_full, y_full = np.array(data[categories][:fraction_int], np.float32), np.array(data[predictors][:fraction_int], np.float32)\n",
        "\n",
        "    split_int = int(len(X_full) * split)\n",
        "    X_train, X_test = X_full[:split_int], X_full[:split_int]\n",
        "    y_train, y_test = y_full[:split_int], y_full[:split_int]\n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "# Generator for windowed data\n",
        "def create_windowed_batches(X_data, y_data, window_size, batch_size, stride=1):\n",
        "    \"\"\"\n",
        "    Generator to create batches of windowed data.\n",
        "    \"\"\"\n",
        "    total_windows = (len(X_data) - window_size) // stride\n",
        "    while True:\n",
        "        for i in range(0, total_windows, batch_size):\n",
        "            X_batch, y_batch = [], []\n",
        "            for j in range(i, min(i + batch_size, total_windows)):\n",
        "                start = j * stride\n",
        "                X_batch.append(X_data[start:start + window_size])\n",
        "                y_batch.append(y_data[start:start + window_size])\n",
        "            yield np.array(X_batch), np.array(y_batch)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "def create_windows(X_data, y_data, window_size, stride=1):\n",
        "    \"\"\"\n",
        "    Create fixed-size windows for evaluation.\n",
        "    \"\"\"\n",
        "    X, y = [], []\n",
        "    for i in range(0, len(X_data) - window_size, stride):\n",
        "        X.append(X_data[i:i + window_size])\n",
        "        y.append(y_data[i:i + window_size])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "def unroll_predictions(y_predict, window_size, stride):\n",
        "    \"\"\"\n",
        "    Unroll the windowed predictions back into a single sequence.\n",
        "    Overlapping regions are averaged.\n",
        "    \"\"\"\n",
        "    unrolled = np.zeros(((y_predict.shape[0] - 1) * stride + window_size, y_predict.shape[2]))\n",
        "    counts = np.zeros(((y_predict.shape[0] - 1) * stride + window_size, 1))\n",
        "\n",
        "    for i in range(y_predict.shape[0]):\n",
        "        start_idx = i * stride\n",
        "        end_idx = start_idx + window_size\n",
        "        unrolled[start_idx:end_idx] += y_predict[i]\n",
        "        counts[start_idx:end_idx] += 1\n",
        "\n",
        "    return unrolled / counts\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ['Voltage', 'Global_intensity', 'Power_factor'],\n",
        "                                                    ['Sub_metering_1', 'Sub_metering_2', 'Sub_metering_3'], split=0.8, fraction=0.1)\n",
        "\n",
        "# Scale the data\n",
        "scaler = StandardScaler()\n",
        "X_train_processed = scaler.fit_transform(X_train)\n",
        "X_test_processed = scaler.transform(X_test)\n",
        "\n",
        "# Define parameters\n",
        "window_size = 256\n",
        "batch_size = 4096\n",
        "stride = 1\n",
        "\n",
        "# Create training and validation generators\n",
        "train_gen = create_windowed_batches(X_train_processed, y_train, window_size, batch_size, stride)\n",
        "val_gen = create_windowed_batches(X_test_processed, y_test, window_size, batch_size, stride)\n",
        "\n",
        "# Calculate the number of steps per epoch\n",
        "train_steps_per_epoch = (len(X_train_processed) - window_size) // stride // batch_size\n",
        "val_steps_per_epoch = (len(X_test_processed) - window_size) // stride // batch_size\n",
        "\n",
        "input_shape = (window_size, X_train_processed.shape[1])\n",
        "\n",
        "model = Sequential([\n",
        "  LSTM(128, input_shape=input_shape, return_sequences=True),\n",
        "  LSTM(128, return_sequences=True),\n",
        "  Dense(32, activation='relu'),\n",
        "  Dense(3)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_mse, metrics=[custom_mae, r2_score])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_gen,\n",
        "    steps_per_epoch=train_steps_per_epoch,\n",
        "    epochs=50,\n",
        "    validation_data=val_gen,\n",
        "    validation_steps=val_steps_per_epoch,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "X_test_windowed, y_test_windowed = create_windows(X_test_processed, y_test, window_size, stride=1)\n",
        "test_loss, test_mae, test_r2 = model.evaluate(X_test_windowed, y_test_windowed, verbose=1)\n",
        "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}, Test R2: {test_r2}\")\n",
        "\n",
        "# Plot training & validation loss\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot MAE\n",
        "plt.plot(history.history['mean_absolute_error'], label='Training MAE')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot R2\n",
        "plt.plot(history.history['r2'], label='Training MAE')\n",
        "plt.plot(history.history['val_mean_absolute_error'], label='Validation MAE')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Mean Absolute Error')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using the best model found, load it in using Keras\n",
        "# Predict using the X_test_windowed, then plot against test data\n",
        "\n",
        "new_model_name = 'LSTM_50_epoch.keras'\n",
        "\n",
        "new_model = tf.keras.models.load_model(new_model_name, compile=True)\n",
        "\n",
        "new_model.summary()\n",
        "\n",
        "y_predict = new_model.predict(X_test_windowed)\n",
        "\n",
        "# Unroll predictions\n",
        "y_predict_unrolled = unroll_predictions(y_predict, window_size=240, stride=15)\n",
        "\n",
        "# Align shapes (trim y_predict_unrolled to match y_test)\n",
        "min_length = min(len(y_predict_unrolled), len(y_test))\n",
        "y_predict_unrolled = y_predict_unrolled[:min_length]\n",
        "y_test_aligned = y_test[:min_length]\n",
        "\n",
        "y_predict_unrolled.shape, y_test_aligned.shape"
      ],
      "metadata": {
        "id": "3Q4qG3jh7xf9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot predictions and true values\n",
        "for i in range(y_predict_unrolled.shape[1]):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(y_test_aligned[:, i], label='True Values', alpha=0.7)\n",
        "    plt.plot(y_predict_unrolled[:, i], label='Predicted Values', alpha=0.7)\n",
        "    plt.title(f'Sub-metering {i+1}')\n",
        "    plt.xlabel('Time')\n",
        "    plt.ylabel('Value')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ei16zjrPJy-o"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}